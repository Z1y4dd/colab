{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4eb9a58",
   "metadata": {},
   "source": [
    "#  Well Log and Laboratory Data Analysis\n",
    "\n",
    "This notebook analyzes the correlation between well log data and laboratory measurements for a well. The analysis includes data loading, cleaning, matching log data with lab samples, and statistical correlation analysis to identify relationships between log measurements and laboratory results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c1781d",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "\n",
    "## Laboratory Data\n",
    "\n",
    "First, we load the laboratory data for well from an Excel file. This data contains various measurements from core samples at specific depths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c12233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlisio import dlis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from pathlib import Path\n",
    "import traceback\n",
    "import glob\n",
    "import sys\n",
    "sys.path.append(r\"c:\\Users\\alghziy\\Desktop\\Analysis\\colab\\HRDH\")\n",
    "\n",
    "# Create 'imgs' folder if it doesn't exist\n",
    "if not os.path.exists('imgs'):\n",
    "    os.makedirs('imgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf481118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading lab_data from an Excel file\n",
    "well_name = 'HRDH_697'\n",
    "lab_data = pd.read_excel(\"../HRDH_LAB_DATA.xlsx\", sheet_name=well_name, index_col='Depth_ft')\n",
    "\n",
    "# convert data types to float\n",
    "lab_data = lab_data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "\n",
    "print(\"\\nDATAFRAME STRUCTURE:\")\n",
    "print(f\"Index: {lab_data.index.name} (shape: {lab_data.index.shape})\")\n",
    "print(f\"Columns: {list(lab_data.columns)} (shape: {lab_data.shape},)\")\n",
    "# print(f\"Data types:\\n{lab_data.info()}\\n\")\n",
    "print(f\"Depth range: {lab_data.index.min():.2f} - {lab_data.index.max():.2f} ft\")\n",
    "\n",
    "lab_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4722df",
   "metadata": {},
   "source": [
    "## Well Log Data\n",
    "\n",
    "Next, we load the well log data from DLIS files. These files contain various log measurements recorded along the wellbore. We import custom functions from the Module.py file to help with loading and processing the DLIS files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4713953",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sys.path.append(r\"c:\\Users\\alghziy\\Desktop\\Analysis\\colab\\HRDH\")\n",
    "from Module import dlis_to_df, load_dlis_files_from_list\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Path(r\"\\\\bhidhares01\\GROUPS\\Daleelah_Interns\\2025 interns\\Ziyad alghamdi\\transtion_minerals\\Log Data\\HRDH\\HRDH_697_0\\8375\\WL\\Field\\Bottom_Section\\Field\\Deliverables\\HRDH_697_0_09JAN2012_ZDL-CN-DSL_ML_BA.dlis\"),\n",
    "14315.0 - 19136.0\n",
    "Path(r\"\\\\bhidhares01\\GROUPS\\Daleelah_Interns\\2025 interns\\Ziyad alghamdi\\transtion_minerals\\Log Data\\HRDH\\HRDH_697_0\\8375\\WL\\Field\\Top_Section\\Field\\Deliverables\\HRDH_697_0_27OCT2011_ZDL-CN-DSL-GR_ML_BA.dlis\"),\n",
    "14516.0 - 14752.5\n",
    "Path(r\"\\\\bhidhares01\\GROUPS\\Daleelah_Interns\\2025 interns\\Ziyad alghamdi\\transtion_minerals\\Log Data\\HRDH\\HRDH_697_0\\12\\WL\\Field\\Deliverables\\HRDH_697_0_15OCT2011_ZDL1-ZDL2-CN-DSL-GR_ML_BA.dlis\"),\n",
    "10855.0 - 14543.8\n",
    "Path(r\"\\\\bhidhares01\\GROUPS\\Daleelah_Interns\\2025 interns\\Ziyad alghamdi\\transtion_minerals\\Log Data\\HRDH\\HRDH_697_0\\12\\WL\\Field\\Deliverables\\HRDH_697_0_15OCT2011_ZDL1-ZDL2-CN-DSL-GR_RP_BA.dlis\")\n",
    "11050.8 - 11467.8\n",
    "\"\"\"\n",
    "# List of DLIS file paths for well \n",
    "dlis_file_paths = [\n",
    "        Path(r\"\\\\bhidhares01\\GROUPS\\Daleelah_Interns\\2025 interns\\Ziyad alghamdi\\transtion_minerals\\Log Data\\HRDH\\HRDH_697_0\\8375\\WL\\Field\\Bottom_Section\\Field\\Deliverables\\HRDH_697_0_09JAN2012_ZDL-CN-DSL_ML_BA.dlis\"),\n",
    "        # 14315.0 - 19136.0\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "log_df, meta = load_dlis_files_from_list(dlis_file_paths) # type: ignore\n",
    "\n",
    "log_df    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9b1159",
   "metadata": {},
   "source": [
    "# Data Cleaning and Quality Assessment\n",
    "\n",
    "In this section, we clean the log data by replacing standard null values with NaN, then assess the data quality by analyzing missing values and other data characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00956b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -999.25 are usually null values inside dlis files\n",
    "null_values = -999.25\n",
    "\n",
    "# Replace -999.25 with NaN FIRST\n",
    "log_df_clean = log_df.replace(null_values, np.nan)\n",
    "log_df_clean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278f92c4",
   "metadata": {},
   "source": [
    "## Log Data Summary\n",
    "\n",
    "Now we'll create a comprehensive summary of the log data to understand its characteristics, including depth range, curve types, data quality, and sampling information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c59a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from Module import create_log_summary\n",
    "\n",
    "\n",
    "# Create a comprehensive summary of the log data\n",
    "summary_stats = create_log_summary(log_df)\n",
    "\n",
    "# Return the summary_stats dictionary for use in subsequent cells\n",
    "summary_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f349b4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missing Data Summary for log data. Calculate missingness summary\n",
    "null_pct = log_df_clean.isna().mean() * 100\n",
    "\n",
    "# Show all columns with their percentages\n",
    "missing_cols = null_pct.sort_values(ascending=False)\n",
    "\n",
    "# Show aggregate summary statistics for many columns\n",
    "print(\"MISSING DATA SUMMARY:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Log DATASET: {log_df_clean.shape[0]} samples Ã— {log_df_clean.shape[1]} curves\")\n",
    "\n",
    "print(f\"Total columns: {len(null_pct)}\")\n",
    "print(f\"Columns with missing data: {len(missing_cols)}\")\n",
    "print(f\"Max missing %: {null_pct.max():.1f}%\")\n",
    "print(f\"Avg missing %: {null_pct.mean():.1f}%\")\n",
    "\n",
    "# Quick check for duplicates\n",
    "# print(\"\\nDUPLICATE DEPTH CHECK:\")\n",
    "# print(f\"Log duplicates: {log_df_clean.index.duplicated().sum()}\")\n",
    "# print(f\"Lab duplicates: {lab_data.index.duplicated().sum()}\")\n",
    "\n",
    "# # Show unique vs total counts\n",
    "# print(f\"\\nLog depths - Total: {len(log_df_clean)}, Unique: {log_df_clean.index.nunique()}\")\n",
    "# print(f\"Lab depths - Total: {len(lab_data)}, Unique: {lab_data.index.nunique()}\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "\n",
    "\n",
    "# Show detailed breakdown for all columns\n",
    "# for col, pct in missing_cols.items():\n",
    "#     count = log_df_clean[col].isna().sum()\n",
    "#     print(f\"â€¢ {col:<20}: {pct:>6.1f}% ({count} missing)\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c855822",
   "metadata": {},
   "source": [
    "# DEPTH ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164c4693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPTH ANALYSIS: Compare sampling depth between log and lab data\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"Log depths summary:\")\n",
    "print(f\"Count: {len(log_df_clean.index)}\")  \n",
    "print(f\"Range: {log_df_clean.index.min():.1f} - {log_df_clean.index.max():.1f} ft\\n\")  \n",
    "log_step = np.diff(log_df_clean.index).mean()  \n",
    "print(f\"Depth step (mean): {log_step:.2f} ft \\nMin Step: {np.diff(log_df_clean.index).min():.2f} \\nMax Step: {np.diff(log_df_clean.index).max():.2f}\")  \n",
    "\n",
    "\n",
    "print(\"=\" * 50)\n",
    "\n",
    "\n",
    "print(\"Lab depths summary:\")\n",
    "print(f\"Count: {len(lab_data.index)}\") \n",
    "print(f\"Range: {lab_data.index.min():.1f} - {lab_data.index.max():.1f} ft\\n\")  \n",
    "lab_step = np.diff(lab_data.index).mean()  \n",
    "print(f\"Depth step (mean): {lab_step:.2f} ft \\nMin Step: {np.diff(lab_data.index).min():.2f} \\nMax Step: {np.diff(lab_data.index).max():.2f}\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f61278",
   "metadata": {},
   "source": [
    "# Depth Overlap Analysis\n",
    "\n",
    "We need to determine where the log and lab data overlap in terms of depth coverage. This is essential for subsequent data integration and correlation analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2626c6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   OVERLAP ANALYSIS \n",
    "print(\"OVERLAP ZONE ANALYSIS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Range\n",
    "print(f\"Depth Ranges:\")\n",
    "print(f\"Log Range: {log_df_clean.index.min():.1f} - {log_df_clean.index.max():.1f} ft\")  \n",
    "print(f\"Lab Range: {lab_data.index.min():.1f} - {lab_data.index.max():.1f} ft\")  \n",
    "\n",
    "# Calculate overlap zone boundaries \n",
    "overlap_start = max(lab_data.index.min(), log_df_clean.index.min())\n",
    "overlap_end = min(lab_data.index.max(), log_df_clean.index.max())\n",
    "overlap_span = overlap_end - overlap_start\n",
    "\n",
    "print(f\"Overlap Range: {overlap_start:.1f} - {overlap_end:.1f} ft\")\n",
    "print(f\"Span of overlap: {overlap_span:.1f} ft\")\n",
    "\n",
    "# Get samples in overlap zone \n",
    "log_overlap = log_df_clean[(log_df_clean.index >= overlap_start) & (log_df_clean.index <= overlap_end)]\n",
    "lab_overlap = lab_data[(lab_data.index >= overlap_start) & (lab_data.index <= overlap_end)]\n",
    "\n",
    "# 2. mean and min/max step\n",
    "if len(log_overlap) > 1:\n",
    "    log_step_overlap = np.diff(log_overlap.index)\n",
    "    print(f\"\\nCORRECTED LOG STEP IN OVERLAP:\")\n",
    "    print(f\"Step (mean): {log_step_overlap.mean():.2f} ft\")\n",
    "    print(f\"Step Min: {log_step_overlap.min():.2f} ft\")\n",
    "    print(f\"Step Max: {log_step_overlap.max():.2f} ft\")  \n",
    "\n",
    "if len(lab_overlap) > 1:\n",
    "    lab_step_overlap = np.diff(lab_overlap.index)\n",
    "    print(f\"\\nCORRECTED LAB STEP IN OVERLAP:\")\n",
    "    print(f\"Step (mean): {lab_step_overlap.mean():.2f} ft\")\n",
    "    print(f\"Step Min: {lab_step_overlap.min():.2f} ft\")\n",
    "    print(f\"Step Max: {lab_step_overlap.max():.2f} ft\")  \n",
    "\n",
    "print(f\"\\nOVERLAP SUMMARY:\")\n",
    "print(f\"Log samples in overlap: {len(lab_overlap):,}\")\n",
    "print(f\"Lab samples in overlap: {len(lab_overlap)}\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dada23e4",
   "metadata": {},
   "source": [
    "# Data Integration\n",
    "\n",
    "In this section, we match the lab data to the log data using a spatial matching approach. This creates depth-matched pairs that can be used for correlation analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ba34af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import cKDTree\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Module import match_lab_to_log\n",
    "\n",
    "# in ft \n",
    "tolerance_ft = 1\n",
    "\n",
    "joined = match_lab_to_log(log_df_clean, lab_overlap, tol=tolerance_ft)\n",
    "\n",
    "if len(joined) > 0:\n",
    "    print(f\"\\nâœ… Found {len(joined)} matches with {tolerance_ft} ft tolerance\")\n",
    "    \n",
    "    # Show statistics\n",
    "    print(f\"\\nMATCH STATISTICS:\")\n",
    "    print(f\"Average distance: {joined['Distance'].mean():.3f} ft\")\n",
    "    print(f\"Max distance: {joined['Distance'].max():.3f} ft\")\n",
    "    print(f\"Exact matches: {(joined['Distance'] == 0).sum()}\")\n",
    "    print(f\"Near matches: {(joined['Distance'] > 0).sum()}\")\n",
    "    \n",
    "    # Save results for verification\n",
    "    verification_df = joined[['Lab_Depth', 'Log_Depth', 'Distance']].copy()\n",
    "    verification_df.to_csv('verification_matches.csv', index=False)\n",
    "    print(f\"\\nSaved verification data to 'verification_matches.csv'\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\nNo matches found with {tolerance_ft} ft tolerance\")\n",
    "    print(\"Trying with larger tolerance...\")\n",
    "    \n",
    "    # Try with diffrent ft tolerance\n",
    "    joined = match_lab_to_log(log_df_clean, lab_overlap, tol=2)\n",
    "\n",
    "joined.to_csv(f\"{well_name}_joined.csv\", index=False)\n",
    "joined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab436ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPREHENSIVE DATA INTEGRATION ASSESSMENT\n",
    "print(\"COMPREHENSIVE DATA INTEGRATION ASSESSMENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Basic Statistics\n",
    "lab_coverage = (len(joined) / len(lab_data)) * 100\n",
    "log_coverage = (len(joined) / len(log_df_clean)) * 100\n",
    "\n",
    "lab_span = lab_data.index.max() - lab_data.index.min()\n",
    "log_span = log_df_clean.index.max() - log_df_clean.index.min()\n",
    "\n",
    "matched_span = joined['Lab_Depth'].max() - joined['Lab_Depth'].min()\n",
    "avg_depth_diff = abs(joined['Log_Depth'] - joined['Lab_Depth']).mean()\n",
    "\n",
    "print(f\"DATA OVERVIEW:\")\n",
    "print(f\"Lab samples: {len(lab_data)} | Log samples: {len(log_df_clean)}\")\n",
    "print(f\"Matched pairs: {len(joined)}\")\n",
    "# print(f\"Data ratio: 1 lab sample per {len(log_df_clean)//len(lab_data)} log samples\") not usful\n",
    "\n",
    "# 2. Coverage Analysis\n",
    "print(f\"\\nCOVERAGE ANALYSIS:\")\n",
    "print(f\"Lab utilization: {lab_coverage:.1f}% ({len(joined)}/{len(lab_data)})\")\n",
    "print(f\"Log coverage: {log_coverage:.1f}% ({len(joined)}/{len(log_df_clean)})\")\n",
    "\n",
    "# 3. Depth Range Analysis\n",
    "print(f\"\\nDEPTH RANGE ANALYSIS:\")\n",
    "print(f\"Lab data span: {lab_span:.1f} ft\")\n",
    "print(f\"Log data span: {log_span:.1f} ft\")\n",
    "print(f\"Matched (joined) span: {matched_span:.1f} ft\")\n",
    "print(f\"Lab covers {(lab_span/log_span)*100:.1f}% of well depth\")\n",
    "\n",
    "# 4. Depth Accuracy\n",
    "print(f\"\\nDEPTH ACCURACY:\")\n",
    "print(f\"Average depth difference: {avg_depth_diff:.2f} ft\")\n",
    "print(f\"Max depth difference: {abs(joined['Log_Depth'] - joined['Lab_Depth']).max():.2f} ft\")\n",
    "print(f\"Min depth difference: {abs(joined['Log_Depth'] - joined['Lab_Depth']).min():.2f} ft\")\n",
    "\n",
    "\n",
    "# 6. Overlap Analysis\n",
    "overlap_start = max(lab_data.index.min(), log_df_clean.index.min())\n",
    "overlap_end = min(lab_data.index.max(), log_df_clean.index.max())\n",
    "overlap_span = overlap_end - overlap_start\n",
    "print(f\"\\nOVERLAP ZONE ANALYSIS:\")\n",
    "print(f\"Overlap zone: {overlap_start:.1f} - {overlap_end:.1f} ft\")\n",
    "\n",
    "# Get samples in overlap zone - FIXED the bug\n",
    "log_overlap = joined[(joined.index >= overlap_start) & (joined.index <= overlap_end)]\n",
    "lab_overlap = lab_data[(lab_data.index >= overlap_start) & (lab_data.index <= overlap_end)]\n",
    "\n",
    "print(f\"Log samples in overlap: {len(log_overlap)}\")\n",
    "print(f\"Lab samples in overlap: {len(lab_overlap)}\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nSUMMARY: Lab utilization is {lab_coverage:.1f}%, but only {log_coverage:.1f}% log coverage.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d25237",
   "metadata": {},
   "source": [
    "# Z-Score Standardization\n",
    "\n",
    "We standardize the data using Z-scores to make different measurements with different units comparable. Z-scores convert values to how many standard deviations they are from the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68060dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Calculate z-scores for joined data to be used in heatmap visualization\n",
    "\n",
    "print(\"Calculating Z-Scores for Joined Data:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Filter for only numeric columns, excluding depth and metadata columns\n",
    "numeric_cols = joined.select_dtypes(include=np.number).columns.tolist()\n",
    "numeric_cols = [col for col in numeric_cols if not any(x in col for x in ['Depth', 'FRAMENO', 'Distance', 'Match_Type'])]\n",
    "\n",
    "# Create a new DataFrame for z-scores\n",
    "joined_z = pd.DataFrame(index=joined.index)\n",
    "\n",
    "# Track skipped columns in a list\n",
    "skipped_cols = []\n",
    "\n",
    "# Calculate z-scores for each numeric column, handling NaN values\n",
    "for col in numeric_cols:\n",
    "    # Get non-NaN values\n",
    "    valid_data = joined[col].dropna()\n",
    "    \n",
    "    if len(valid_data) > 1:  # Need at least 2 values to calculate z-score\n",
    "        # Calculate z-scores only for non-NaN values\n",
    "        z_scores = stats.zscore(valid_data)\n",
    "        \n",
    "        # Create a Series with NaN values where original data had NaNs\n",
    "        z_col = pd.Series(index=joined.index, dtype=float)\n",
    "        z_col.loc[valid_data.index] = z_scores\n",
    "        \n",
    "        # Add to z-score DataFrame\n",
    "        joined_z[col] = z_col\n",
    "    else:\n",
    "        # Add to skipped columns list instead of printing individually\n",
    "        skipped_cols.append(col)\n",
    "\n",
    "# Add depth column for reference\n",
    "joined_z['Depth'] = joined['Lab_Depth']\n",
    "\n",
    "# Print summary of results\n",
    "print(f\"Z-Score calculation complete for {len(numeric_cols) - len(skipped_cols)} variables\")\n",
    "print(f\"Shape: {joined_z.shape}\")\n",
    "print(f\"Variables standardized to Î¼=0, Ïƒ=1 scale\")\n",
    "print(f\"NaN values preserved from original data\")\n",
    "\n",
    "# Print skipped columns summary\n",
    "if skipped_cols:\n",
    "    print(f\"\\nSkipped {len(skipped_cols)} columns with insufficient data points:\")\n",
    "    print(f\"Skipped columns: {skipped_cols}\")\n",
    "\n",
    "# Display first few rows\n",
    "joined_z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82ced82",
   "metadata": {},
   "source": [
    "## Z-Score Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32bd8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Filter for measurement columns in joined_z (excluding metadata)\n",
    "z_lab_vars = [col for col in joined_z.columns if col.startswith('Lab_') and \n",
    "             col not in ['Lab_Depth', 'Lab_Sample_ID', 'Depth']]\n",
    "\n",
    "z_log_vars = [col for col in joined_z.columns if col.startswith('Log_') and \n",
    "             col not in ['Log_Depth', 'Log_FRAMENO', 'Depth']]\n",
    "\n",
    "# Filter out columns where all values are 0 or NaN\n",
    "z_lab_vars = [col for col in z_lab_vars if not (joined_z[col] == 0).all() and not joined_z[col].isna().all()]\n",
    "z_log_vars = [col for col in z_log_vars if not (joined_z[col] == 0).all() and not joined_z[col].isna().all()]\n",
    "\n",
    "# ALSO filter out constant columns\n",
    "z_lab_vars = [col for col in z_lab_vars if joined_z[col].std() > 0]\n",
    "z_log_vars = [col for col in z_log_vars if joined_z[col].std() > 0]\n",
    "\n",
    "# Print variable counts\n",
    "print(len(z_lab_vars), \"lab variables (z-scores)\")\n",
    "print(len(z_log_vars), \"log variables (z-scores)\")\n",
    "\n",
    "# Compute correlation matrix (log variables as rows, lab variables as columns)\n",
    "z_corr_matrix = joined_z[z_lab_vars + z_log_vars].corr().loc[z_log_vars, z_lab_vars]\n",
    "\n",
    "# Create a larger figure with additional left margin for y-labels\n",
    "plt.figure(figsize=(24, 24))\n",
    "\n",
    "# Add more space on the left side of the plot\n",
    "plt.subplots_adjust(left=0.2)  # Increase left margin to make room for y-labels\n",
    "\n",
    "# Plot the heatmap with improved label visibility\n",
    "hm = sns.heatmap(z_corr_matrix, \n",
    "            annot=True,                  # Show correlation values\n",
    "            cmap='RdYlGn',               # Use a diverging colormap\n",
    "            vmin=-1, vmax=1,             # Fixed scale for correlation values\n",
    "            linewidths=0.5,              # Add line separators\n",
    "            fmt='.2f',                   # Format as 2 decimal places\n",
    "            annot_kws={'size': 20},      # Annotation font size\n",
    "            cbar_kws={\"shrink\": 0.8})    # Adjust colorbar\n",
    "\n",
    "# Labeling and styling\n",
    "plt.title('Z-Score Correlation Between Log Measurements (y-axis) and Lab Measurements (x-axis)', fontsize=20)\n",
    "\n",
    "# Improve x-axis label formatting\n",
    "plt.xticks(rotation=45, ha='right', fontsize=20)\n",
    "\n",
    "# Improve y-axis label formatting - make them more visible\n",
    "plt.yticks(fontsize=20, rotation=0)  # Horizontal labels (0 degrees rotation)\n",
    "\n",
    "# Add clearer axis labels\n",
    "plt.xlabel('Lab Measurements (Z-Scores)', fontsize=25)\n",
    "plt.ylabel('Log Measurements (Z-Scores)', fontsize=25)\n",
    "\n",
    "# Apply tight_layout AFTER subplots_adjust to prevent overrides\n",
    "plt.tight_layout()\n",
    "\n",
    "# Move the y-label to be more visible\n",
    "plt.gcf().axes[0].yaxis.set_label_coords(-0.15, 0.5)  # Adjust position as needed\n",
    "\n",
    "# Save with bigger bbox_inches to ensure nothing gets cut off\n",
    "plt.savefig(f'imgs/{well_name}_zscore_correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Add a summary of the strongest correlations\n",
    "print(\"\\nSTRONGEST Z-SCORE CORRELATIONS:\")\n",
    "\n",
    "# Get the top 5 positive correlations\n",
    "top_pos = z_corr_matrix.unstack().sort_values(ascending=False)[~z_corr_matrix.unstack().index.duplicated(keep='first')][:5] # type: ignore\n",
    "print(\"Top positive correlations:\")\n",
    "for (log_var, lab_var), corr in top_pos.items(): # type: ignore\n",
    "    print(f\"{log_var} â†” {lab_var}: r = {corr:.3f}\")\n",
    "\n",
    "# Get the top 5 negative correlations\n",
    "top_neg = z_corr_matrix.unstack().sort_values()[~z_corr_matrix.unstack().index.duplicated(keep='first')][:5] # type: ignore\n",
    "print(\"\\nTop negative correlations:\")\n",
    "for (log_var, lab_var), corr in top_neg.items(): # type: ignore\n",
    "    print(f\"{log_var} â†” {lab_var}: r = {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b71211",
   "metadata": {},
   "source": [
    "## Z-Score enhanced Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6f0e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-SCORE ENHANCED CORRELATION ANALYSIS\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from Module import create_zscore_enhanced_heatmap, enhanced_zscore_correlation_analysis\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Get z-score variables\n",
    "z_lab_vars = [col for col in joined_z.columns if col.startswith('Lab_') and \n",
    "            col not in ['Lab_Depth', 'Lab_Sample_ID', 'Depth']]\n",
    "\n",
    "z_log_vars = [col for col in joined_z.columns if col.startswith('Log_') and \n",
    "            col not in ['Log_Depth', 'Log_FRAMENO', 'Depth']]\n",
    "\n",
    "# Filter out columns where all values are 0 or NaN\n",
    "z_lab_vars = [col for col in z_lab_vars if not (joined_z[col] == 0).all() and not joined_z[col].isna().all()]\n",
    "z_log_vars = [col for col in z_log_vars if not (joined_z[col] == 0).all() and not joined_z[col].isna().all()]\n",
    "\n",
    "# ALSO filter out constant columns\n",
    "z_lab_vars = [col for col in z_lab_vars if joined_z[col].std() > 0]\n",
    "z_log_vars = [col for col in z_log_vars if joined_z[col].std() > 0]\n",
    "\n",
    "\n",
    "print(f\"Z-score variables for analysis: {len(z_log_vars)} log vars, {len(z_lab_vars)} lab vars\")\n",
    "\n",
    "# Run enhanced correlation analysis on z-scores\n",
    "z_correlation_results = enhanced_zscore_correlation_analysis(joined_z, z_log_vars, z_lab_vars)\n",
    "\n",
    "# Create enhanced correlation heatmap for z-scores\n",
    "create_zscore_enhanced_heatmap(z_correlation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7264a458",
   "metadata": {},
   "source": [
    "# Visualization of Results\n",
    "\n",
    "## Match Distances Plot\n",
    "\n",
    "This visualization shows the distribution of depth differences between matched log and lab data points, which helps assess the quality of our spatial matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faf5800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the quality of matches\n",
    "from Module import visualize_match_quality\n",
    "\n",
    "        \n",
    "visualize_match_quality(joined, well_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abfe644",
   "metadata": {},
   "source": [
    "## Correlation Heatmap\n",
    "\n",
    "This heatmap visualizes the strength of correlations between log and lab measurements, with color intensity indicating correlation strength and direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf91da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Filter for all lab measurement columns (excluding non-measurement columns)\n",
    "lab_vars = [col for col in joined.columns if col.startswith('Lab_') and \n",
    "           col not in ['Lab_Depth', 'Lab_Sample_ID']]\n",
    "\n",
    "# Filter for all log measurement columns (excluding non-measurement or auxiliary columns)\n",
    "# Comment out the original code that selects all log variables\n",
    "log_vars = [col for col in joined.columns if col.startswith('Log_') and \n",
    "           col not in ['Log_Depth', 'Log_FRAMENO']]\n",
    "\n",
    "# Only include specific log measurements\n",
    "specific_logs = ['CN', 'CNC', 'GR', 'GRSL', 'HRD1', 'HRD2', 'K', 'KTH', 'LSN', \n",
    "                'PE', 'QPKS', 'SFT2', 'SHR', 'SLTM', 'ZDNC']\n",
    "log_vars = [f'Log_{log}' for log in specific_logs if f'Log_{log}' in joined.columns]\n",
    "\n",
    "# Filter out columns where all values are 0 or NaN\n",
    "lab_vars = [col for col in lab_vars if not (joined[col] == 0).all() and not joined[col].isna().all()]\n",
    "log_vars = [col for col in log_vars if not (joined[col] == 0).all() and not joined[col].isna().all()]\n",
    "\n",
    "# ALSO filter out constant columns\n",
    "lab_vars = [col for col in lab_vars if joined[col].std() > 0]\n",
    "log_vars = [col for col in log_vars if joined[col].std() > 0]  # FIXED: Now filtering log_vars properly\n",
    "\n",
    "# Print variable counts for verification\n",
    "print(f\"{len(lab_vars)} lab variables\")\n",
    "print(f\"{len(log_vars)} log variables\")\n",
    "\n",
    "# Compute correlation matrix (log variables as rows, lab variables as columns)\n",
    "corr_matrix = joined[lab_vars + log_vars].corr().loc[log_vars, lab_vars]\n",
    "\n",
    "# Create a larger figure for better readability\n",
    "plt.figure(figsize=(24, 24))\n",
    "\n",
    "# Plot the heatmap with rotated x-axis labels\n",
    "sns.heatmap(corr_matrix, \n",
    "            annot=True,                  # Show correlation values\n",
    "            cmap='RdYlGn',               # Use a diverging colormap\n",
    "            vmin=-1, vmax=1,             # Fixed scale for correlation values\n",
    "            linewidths=0.5,              # Add line separators\n",
    "            fmt='.2f',                   # Format as 2 decimal places\n",
    "            annot_kws={'size': 20},       # Smaller annotation font size\n",
    "            cbar_kws={\"shrink\": 0.8})    # Adjust colorbar\n",
    "            \n",
    "\n",
    "#  labeling and styling\n",
    "plt.title('Correlation Between Log Measurements (y-axis) and Lab Measurements (x-axis)', fontsize=20)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=20)      \n",
    "plt.yticks(rotation=0, fontsize=20)                   \n",
    "# Add clearer axis labels\n",
    "plt.xlabel('Lab Measurements', fontsize=25, )\n",
    "plt.ylabel('Log Measurements', fontsize=25, )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'imgs/{well_name}_correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51b1e4f",
   "metadata": {},
   "source": [
    "## Enhanced Correlation Heatmap\n",
    "\n",
    "This enhanced heatmap includes statistical significance information, highlighting only correlations that pass our significance threshold (p â‰¤ 0.05)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3da8587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENHANCED CORRELATION ANALYSIS (non-Z-score version)\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from Module import create_enhanced_correlation_heatmap, enhanced_correlation_analysis\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Get regular variables (not z-scores)\n",
    "lab_vars = [col for col in joined.columns if col.startswith('Lab_') and \n",
    "           col not in ['Lab_Depth', 'Lab_Sample_ID']]\n",
    "\n",
    "log_vars = [col for col in joined.columns if col.startswith('Log_') and \n",
    "           col not in ['Log_Depth', 'Log_FRAMENO']]\n",
    "\n",
    "# Filter out columns where all values are 0 or NaN\n",
    "lab_vars = [col for col in lab_vars if not (joined[col] == 0).all() and not joined[col].isna().all()]\n",
    "log_vars = [col for col in log_vars if not (joined[col] == 0).all() and not joined[col].isna().all()]\n",
    "\n",
    "# ALSO filter out constant columns\n",
    "lab_vars = [col for col in lab_vars if joined[col].std() > 0]\n",
    "log_vars = [col for col in log_vars if joined[col].std() > 0]\n",
    "\n",
    "print(f\"Variables for analysis: {len(log_vars)} log vars, {len(lab_vars)} lab vars\")\n",
    "\n",
    "# Run enhanced correlation analysis on regular (non-z-score) data\n",
    "correlation_results = enhanced_correlation_analysis(joined, log_vars, lab_vars)\n",
    "\n",
    "# Create enhanced correlation heatmap for regular correlations\n",
    "create_enhanced_correlation_heatmap(correlation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22364d82",
   "metadata": {},
   "source": [
    "## Scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffe8a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZE SIGNIFICANT CORRELATIONS WITH SCATTER PLOTS\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "from Module import visualize_significant_correlations, create_correlation_figure\n",
    "\n",
    "\n",
    "# Print header\n",
    "print(\"ðŸ“Š VISUALIZING SIGNIFICANT CORRELATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Set parameters for correlation visualization\n",
    "significance_level = 0.05  # p-value threshold\n",
    "min_correlation = 0.4      # minimum correlation strength to display\n",
    "\n",
    "# Call the function to visualize significant correlations\n",
    "results = visualize_significant_correlations(\n",
    "    joined, \n",
    "    correlation_results,\n",
    "    significance_level=significance_level,\n",
    "    min_correlation=min_correlation\n",
    ")\n",
    "\n",
    "# Create a detailed visualization for the strongest correlation\n",
    "if results['positive_correlations'] or results['negative_correlations']:\n",
    "    # Find the strongest correlation (positive or negative)\n",
    "    all_correlations = results['positive_correlations'] + results['negative_correlations']\n",
    "    if all_correlations:\n",
    "        # Sort by absolute correlation value\n",
    "        strongest = max(all_correlations, key=lambda x: abs(x[2]))\n",
    "        log_var, lab_var, r, p, n = strongest\n",
    "        \n",
    "        print(f\"\\nðŸ” DETAILED ANALYSIS OF STRONGEST CORRELATION:\")\n",
    "        print(f\"Log Variable: {log_var}\")\n",
    "        print(f\"Lab Variable: {lab_var}\")\n",
    "        print(f\"Pearson's r: {r:.3f} (p-value: {p:.4f}, n={n})\")\n",
    "        \n",
    "\n",
    "\n",
    "print(\"\\nâœ… Correlation visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dce3dd",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fb3f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import pandas as pd\n",
    "from Module import create_correlation_network\n",
    "\n",
    "\n",
    "\n",
    "# Use the function with your correlation results\n",
    "create_correlation_network(correlation_results, min_correlation=0.75, max_connections=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c1befb",
   "metadata": {},
   "source": [
    "## Distribution of Pearson's Correlation Coefficients\n",
    "\n",
    "This visualization shows the distribution of all Pearson correlation coefficients (r-values) between log and lab measurements, helping us understand the overall relationship patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb92579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all unique correlation coefficients\n",
    "vals = z_corr_matrix.values\n",
    "mask = np.triu(np.ones_like(vals), k=1).astype(bool)\n",
    "r_vals = vals[mask]\n",
    "\n",
    "# Compute comprehensive statistics about the correlation distribution\n",
    "mean_r = np.mean(r_vals)\n",
    "median_r = np.median(r_vals)\n",
    "min_r = np.min(r_vals)\n",
    "max_r = np.max(r_vals)\n",
    "std_r = np.std(r_vals)\n",
    "\n",
    "# Count correlations by strength category\n",
    "strong_pos_count = np.sum(r_vals > 0.6)\n",
    "strong_neg_count = np.sum(r_vals < -0.6)\n",
    "moderate_count = np.sum((np.abs(r_vals) > 0.3) & (np.abs(r_vals) <= 0.6))\n",
    "weak_count = np.sum(np.abs(r_vals) <= 0.3)\n",
    "\n",
    "# Create descriptive text for the plot\n",
    "stats_text = (\n",
    "    f\"Statistics:\\n\"\n",
    "    f\"Mean: {mean_r:.2f}\\n\"\n",
    "    f\"Median: {median_r:.2f}\\n\"\n",
    "    f\"Std Dev: {std_r:.2f}\\n\"\n",
    "    f\"Range: [{min_r:.2f}, {max_r:.2f}]\\n\\n\"\n",
    "    f\"Correlation Strength:\\n\"\n",
    "    f\"Strong positive (>0.6): {strong_pos_count}\\n\"\n",
    "    f\"Strong negative (<-0.6): {strong_neg_count}\\n\"\n",
    "    f\"Moderate (0.3-0.6): {moderate_count}\\n\"\n",
    "    f\"Weak (<0.3): {weak_count}\"\n",
    ")\n",
    "\n",
    "# Create a better visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Main histogram with KDE\n",
    "ax = plt.subplot(111)\n",
    "sns.histplot(r_vals, bins=25, kde=True, color='skyblue', \n",
    "             line_kws={'linewidth': 2}, alpha=0.7)\n",
    "\n",
    "# Add vertical lines for reference\n",
    "plt.axvline(x=0, color='gray', linestyle='--', alpha=0.7)\n",
    "plt.axvline(x=mean_r, color='red', linestyle='-', label=f'Mean ({mean_r:.2f})')\n",
    "plt.axvline(x=median_r, color='green', linestyle='-', label=f'Median ({median_r:.2f})')\n",
    "\n",
    "# Shade regions by correlation strength\n",
    "plt.axvspan(-1, -0.6, alpha=0.2, color='red', label='Strong negative')\n",
    "plt.axvspan(-0.6, -0.3, alpha=0.1, color='orange', label='Moderate negative')\n",
    "plt.axvspan(-0.3, 0.3, alpha=0.1, color='gray', label='Weak')\n",
    "plt.axvspan(0.3, 0.6, alpha=0.1, color='blue', label='Moderate positive')\n",
    "plt.axvspan(0.6, 1, alpha=0.2, color='green', label='Strong positive')\n",
    "\n",
    "# Add statistics textbox\n",
    "plt.text(1.02, 0.5, stats_text, transform=ax.transAxes, fontsize=9,\n",
    "         verticalalignment='center', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "# Improve labels and title\n",
    "plt.title(\"Distribution of Pearson Correlation Coefficients\", fontsize=14)\n",
    "plt.xlabel(\"Correlation Coefficient (r)\", fontsize=12)\n",
    "plt.ylabel(\"Frequency\", fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(loc='upper left', fontsize=9)\n",
    "\n",
    "# Adjust layout to make room for the text box\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(right=0.75)\n",
    "\n",
    "# Save high-quality figure\n",
    "plt.savefig(f'imgs/{well_name}correlation_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print summary for notebook\n",
    "print(\"\\nCORRELATION DISTRIBUTION SUMMARY:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total correlations analyzed: {len(r_vals)}\")\n",
    "print(f\"Average correlation: {mean_r:.2f} (median: {median_r:.2f})\")\n",
    "print(f\"Strong correlations (|r| > 0.6): {strong_pos_count + strong_neg_count} ({(strong_pos_count + strong_neg_count)/len(r_vals)*100:.1f}%)\")\n",
    "print(f\"Positive skew: {strong_pos_count/(strong_pos_count + strong_neg_count)*100:.1f}% of strong correlations are positive\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c74bba",
   "metadata": {},
   "source": [
    "## mineral_composition_bar, depth_trend_plots, composite_log_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b21b5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Module import create_mineral_composition_bar, create_depth_trend_plots, create_composite_log_plot\n",
    "# Define mineral variables (XRD columns)\n",
    "mineral_vars = [col for col in joined.columns if 'XRD' in col]\n",
    "\n",
    "# Define log variables\n",
    "log_vars = [col for col in joined.columns if col.startswith('Log_') and \n",
    "           col not in ['Log_Depth', 'Log_FRAMENO']]\n",
    "\n",
    "# 1. Create mineral composition bar chart\n",
    "print(\"\\n CREATING MINERAL COMPOSITION CHART...\")\n",
    "if len(mineral_vars) >= 3:\n",
    "    create_mineral_composition_bar(joined, mineral_vars)\n",
    "\n",
    "# 3. Create depth trend plots for selected variables\n",
    "print(\"\\n CREATING DEPTH TREND PLOTS...\")\n",
    "key_vars = ['Log_GR', 'Log_ZDEN', 'Lab_XRD_Quartz']\n",
    "key_vars = [var for var in key_vars if var in joined.columns]\n",
    "if len(key_vars) >= 1:\n",
    "    create_depth_trend_plots(joined, key_vars)\n",
    "\n",
    "\n",
    "# 5. Create composite log plot\n",
    "print(\"\\n CREATING COMPOSITE LOG PLOT...\")\n",
    "create_composite_log_plot(joined, log_vars)\n",
    "\n",
    "print(\"\\nâœ… ALL VISUALIZATIONS CREATED AND SAVED!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31200419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZATIONS - Efficient grid plots \n",
    "from Module import create_distribution_grid\n",
    "print(\"Creating efficient distribution visualizations...\")\n",
    "\n",
    "# Split variables into logical groups for better visualization\n",
    "log_measurement_vars = [v for v in log_vars if any(x in v for x in ['GR', 'ZDEN', 'CN', 'PE', 'U', 'TH', 'K'])]\n",
    "lab_xrd_vars = [v for v in lab_vars if 'XRD' in v]\n",
    "lab_xrf_vars = [v for v in lab_vars if 'XRF' in v]\n",
    "\n",
    "\n",
    "# Create grouped visualizations with standardized naming convention\n",
    "if log_measurement_vars:\n",
    "    fig1 = create_distribution_grid(joined, log_measurement_vars, ncols=3)\n",
    "    fig1.suptitle(f\"{well_name} Log Measurements Distribution\", fontsize=16, y=1.02)\n",
    "    plt.savefig(f'imgs/distributions_log_{well_name}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "if lab_xrd_vars:\n",
    "    fig2 = create_distribution_grid(joined, lab_xrd_vars, ncols=3)\n",
    "    fig2.suptitle(f\"{well_name} XRD Mineralogy Distribution\", fontsize=16, y=1.02)\n",
    "    plt.savefig(f'imgs/distributions_xrd_{well_name}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "if lab_xrf_vars:\n",
    "    fig3 = create_distribution_grid(joined, lab_xrf_vars[:12], ncols=4)  # Limit to first 12 for readability\n",
    "    fig3.suptitle(f\"{well_name} XRF Elements Distribution (Top 12)\", fontsize=16, y=1.02)\n",
    "    plt.savefig(f'imgs/distributions_xrf_{well_name}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "print(\"âœ… Distributions visualizations completed and saved with standardized naming!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41131594",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3f9eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Compute summary stats\n",
    "stats = joined[log_vars + lab_vars].describe().T\n",
    "stats['skew']    = joined[stats.index].skew()\n",
    "stats['kurtosis']= joined[stats.index].kurtosis()\n",
    "\n",
    "#  Display top 10 for review\n",
    "stats[['mean','50%','std','min','max','skew','kurtosis']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
