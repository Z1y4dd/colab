{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b9dda60",
   "metadata": {},
   "source": [
    "Your mentor is suggesting **Min-Max standardization** for good reasons! Let me explain **why**, **where**, and **when** it would be beneficial for your geological data analysis.\n",
    "\n",
    "## Why Min-Max Standardization is Important for Your Data:\n",
    "\n",
    "### **1. Different Measurement Scales**\n",
    "Your well log and lab data have vastly different scales:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5ef50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example scales in your data:\n",
    "Log_GR: 0-150 API units (gamma ray)\n",
    "Log_PE: 1-6 barns/electron (photoelectric factor)  \n",
    "Lab_SiO2: 45-75% (silica percentage)\n",
    "Lab_Fe2O3: 0.1-15% (iron oxide percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6199a3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **2. Correlation Bias Toward Large-Scale Variables**\n",
    "Without standardization:\n",
    "- Variables with larger ranges (like GR: 0-150) can dominate correlations\n",
    "- Variables with smaller ranges (like PE: 1-6) may appear less correlated\n",
    "- This creates **artificial bias** in your correlation analysis\n",
    "\n",
    "## Where to Apply Min-Max Standardization:\n",
    "\n",
    "### **Option 1: Before Correlation Calculation (Recommended)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83db5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def calculate_well_correlations_standardized(df, log_columns, lab_columns, min_samples=10):\n",
    "    \"\"\"Calculate correlations using Min-Max standardized data\"\"\"\n",
    "    correlations = {}\n",
    "    \n",
    "    for log_var in log_columns:\n",
    "        for lab_var in lab_columns:\n",
    "            # Get valid data\n",
    "            mask = (~df[log_var].isna()) & (~df[lab_var].isna())\n",
    "            \n",
    "            if mask.sum() >= min_samples:\n",
    "                # Extract data\n",
    "                log_data = df.loc[mask, log_var].values\n",
    "                lab_data = df.loc[mask, lab_var].values\n",
    "                \n",
    "                # Min-Max standardization (0 to 1 scale)\n",
    "                scaler_log = MinMaxScaler()\n",
    "                scaler_lab = MinMaxScaler()\n",
    "                \n",
    "                log_standardized = scaler_log.fit_transform(log_data.reshape(-1, 1)).flatten()\n",
    "                lab_standardized = scaler_lab.fit_transform(lab_data.reshape(-1, 1)).flatten()\n",
    "                \n",
    "                # Calculate correlation on standardized data\n",
    "                try:\n",
    "                    r, p = pearsonr(log_standardized, lab_standardized)\n",
    "                    if not np.isnan(r):\n",
    "                        correlations[(log_var, lab_var)] = {\n",
    "                            'correlation': r,\n",
    "                            'p_value': p,\n",
    "                            'n_samples': len(log_data),\n",
    "                            'log_range': (log_data.min(), log_data.max()),\n",
    "                            'lab_range': (lab_data.min(), lab_data.max())\n",
    "                        }\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    return correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1fdd58",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Option 2: For Visualization (Alternative)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1163249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_standardized_scatter_plot(df, log_var, lab_var, well_colors):\n",
    "    \"\"\"Create scatter plot with standardized axes but show original values\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Plot 1: Original data\n",
    "    for well in df['Well'].unique():\n",
    "        well_data = df[df['Well'] == well]\n",
    "        mask = (~well_data[log_var].isna()) & (~well_data[lab_var].isna())\n",
    "        \n",
    "        if mask.sum() > 0:\n",
    "            ax1.scatter(well_data.loc[mask, log_var], \n",
    "                       well_data.loc[mask, lab_var],\n",
    "                       color=well_colors.get(well, '#666666'), \n",
    "                       alpha=0.6, label=well.replace('HRDH_', ''))\n",
    "    \n",
    "    ax1.set_xlabel(f'{log_var} (original scale)')\n",
    "    ax1.set_ylabel(f'{lab_var} (original scale)')\n",
    "    ax1.set_title('Original Data Scale')\n",
    "    \n",
    "    # Plot 2: Standardized data\n",
    "    scaler_x = MinMaxScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "    \n",
    "    for well in df['Well'].unique():\n",
    "        well_data = df[df['Well'] == well]\n",
    "        mask = (~well_data[log_var].isna()) & (~well_data[lab_var].isna())\n",
    "        \n",
    "        if mask.sum() > 0:\n",
    "            x_orig = well_data.loc[mask, log_var].values\n",
    "            y_orig = well_data.loc[mask, lab_var].values\n",
    "            \n",
    "            x_std = scaler_x.fit_transform(x_orig.reshape(-1, 1)).flatten()\n",
    "            y_std = scaler_y.fit_transform(y_orig.reshape(-1, 1)).flatten()\n",
    "            \n",
    "            ax2.scatter(x_std, y_std,\n",
    "                       color=well_colors.get(well, '#666666'), \n",
    "                       alpha=0.6, label=well.replace('HRDH_', ''))\n",
    "    \n",
    "    ax2.set_xlabel(f'{log_var} (0-1 standardized)')\n",
    "    ax2.set_ylabel(f'{lab_var} (0-1 standardized)')\n",
    "    ax2.set_title('Min-Max Standardized Data')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c8e9f6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## When Standardization is Most Beneficial:\n",
    "\n",
    "### **1. Cross-Variable Correlation Comparisons**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3bba33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without standardization: misleading comparison\n",
    "GR vs SiO2: r = 0.65 (GR range: 0-150, SiO2 range: 45-75)\n",
    "PE vs Fe2O3: r = 0.45 (PE range: 1-6, Fe2O3 range: 0.1-15)\n",
    "\n",
    "# With standardization: fair comparison\n",
    "GR vs SiO2: r = 0.58 (both scaled 0-1)\n",
    "PE vs Fe2O3: r = 0.52 (both scaled 0-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ab1587",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **2. Heatmap Improvements**\n",
    "Your heatmap would show more balanced correlations:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38883a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_standardized_correlation_heatmap(df_all, lab_columns, log_columns):\n",
    "    \"\"\"Create heatmap using standardized correlations\"\"\"\n",
    "    \n",
    "    # Standardize all data first\n",
    "    scaler = MinMaxScaler()\n",
    "    df_standardized = df_all.copy()\n",
    "    \n",
    "    # Standardize log columns\n",
    "    for col in log_columns:\n",
    "        valid_data = df_all[col].dropna()\n",
    "        if len(valid_data) > 0:\n",
    "            df_standardized[col] = scaler.fit_transform(df_all[[col]])\n",
    "    \n",
    "    # Standardize lab columns  \n",
    "    for col in lab_columns:\n",
    "        valid_data = df_all[col].dropna()\n",
    "        if len(valid_data) > 0:\n",
    "            df_standardized[col] = scaler.fit_transform(df_all[[col]])\n",
    "    \n",
    "    # Calculate correlations on standardized data\n",
    "    correlation_matrix = df_standardized[log_columns + lab_columns].corr()\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    sns.heatmap(correlation_matrix.loc[lab_columns, log_columns],\n",
    "                annot=True, cmap='RdBu_r', center=0, vmin=-1, vmax=1)\n",
    "    plt.title('Correlation Heatmap (Min-Max Standardized Data)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return correlation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26781dd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Recommended Implementation for Your Code:\n",
    "\n",
    "Update your notebook cell:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2759793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this to your scatterplot cell\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create standardized correlation analysis\n",
    "print(\"Creating standardized correlation analysis...\")\n",
    "\n",
    "# Option 1: Compare standardized vs original correlations\n",
    "def compare_correlation_methods(df_all, correlations_by_well_count):\n",
    "    \"\"\"Compare original vs standardized correlations\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for n_wells in correlations_by_well_count:\n",
    "        for pair, wells_data, info in correlations_by_well_count[n_wells]:\n",
    "            log_var, lab_var = pair\n",
    "            \n",
    "            # Get data from all wells with this correlation\n",
    "            well_names = [w for w, _ in wells_data]\n",
    "            mask = df_all['Well'].isin(well_names)\n",
    "            combined_data = df_all[mask][[log_var, lab_var]].dropna()\n",
    "            \n",
    "            if len(combined_data) > 10:\n",
    "                # Original correlation\n",
    "                orig_r, _ = pearsonr(combined_data[log_var], combined_data[lab_var])\n",
    "                \n",
    "                # Standardized correlation\n",
    "                scaler_log = MinMaxScaler()\n",
    "                scaler_lab = MinMaxScaler()\n",
    "                \n",
    "                log_std = scaler_log.fit_transform(combined_data[[log_var]]).flatten()\n",
    "                lab_std = scaler_lab.fit_transform(combined_data[[lab_var]]).flatten()\n",
    "                \n",
    "                std_r, _ = pearsonr(log_std, lab_std)\n",
    "                \n",
    "                results.append({\n",
    "                    'log_var': log_var,\n",
    "                    'lab_var': lab_var,\n",
    "                    'n_wells': n_wells,\n",
    "                    'original_r': orig_r,\n",
    "                    'standardized_r': std_r,\n",
    "                    'difference': abs(std_r) - abs(orig_r),\n",
    "                    'log_range': combined_data[log_var].max() - combined_data[log_var].min(),\n",
    "                    'lab_range': combined_data[lab_var].max() - combined_data[lab_var].min()\n",
    "                })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(results)\n",
    "    return comparison_df\n",
    "\n",
    "# Run the comparison\n",
    "comparison_results = compare_correlation_methods(df_all, correlations_by_well_count)\n",
    "\n",
    "# Show cases where standardization makes the biggest difference\n",
    "print(\"\\nTop 10 cases where standardization changes correlation most:\")\n",
    "top_changes = comparison_results.nlargest(10, 'difference')\n",
    "for _, row in top_changes.iterrows():\n",
    "    print(f\"{row['log_var'].replace('Log_', '')} vs {row['lab_var'].replace('Lab_', '')}: \"\n",
    "          f\"Original r={row['original_r']:.3f}, Standardized r={row['standardized_r']:.3f}, \"\n",
    "          f\"Difference={row['difference']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee143cf",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Benefits You'll See:\n",
    "\n",
    "1. **More accurate cross-variable comparisons**\n",
    "2. **Better identification of true geological relationships**\n",
    "3. **Reduced bias toward variables with larger numerical ranges**\n",
    "4. **More balanced heatmap visualizations**\n",
    "5. **Better reproducibility across different measurement units**\n",
    "\n",
    "This is especially important in geology where you're comparing physical properties measured in completely different units (API units, percentages, densities, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d655ee22",
   "metadata": {},
   "source": [
    "Looking at your code, if you want to apply Min-Max normalization, there are several strategic places where you could implement it, each with different effects on your results. Let me show you the main approaches:\n",
    "\n",
    "## Option 1: Apply Normalization During Correlation Calculation (Recommended)\n",
    "\n",
    "The most impactful place would be in the `calculate_well_correlations()` function in your `All_well_analysis.py` file. Here's how to modify it:\n",
    "\n",
    "\n",
    "\n",
    "Show in Settings Editor\n",
    "\n",
    "\n",
    "\n",
    "**Modified function:**\n",
    "\n",
    "\n",
    "\n",
    "## Option 2: Create a Separate Normalization Analysis Function\n",
    "\n",
    "Add this function to your notebook to compare normalized vs original correlations:\n",
    "\n",
    " 3: Apply Normalization in Heatmap Creation\n",
    "\n",
    "Modify the `create_combined_correlation_heatmap()` function:\n",
    "\n",
    " How Normalization Will Affect Your Results:\n",
    "\n",
    "### 1. **Correlation Values Will Change**\n",
    "- Variables with different scales will have more comparable correlations\n",
    "- Some weak correlations may become stronger\n",
    "- Some strong correlations may become weaker\n",
    "- The relative ranking of correlations may change\n",
    "\n",
    "### 2. **Impact on Your Analysis Pipeline**\n",
    "- **Common correlations**: Different pairs may meet your threshold\n",
    "- **Heatmaps**: Color patterns will change\n",
    "- **Scatter plots**: The actual correlation values displayed will be different\n",
    "- **Summary statistics**: Counts of strong/moderate/weak correlations will change\n",
    "\n",
    "### 3. **Your Current Min-Max Analysis**\n",
    "Your existing comparison function shows this is already working - you found that:\n",
    "- Some correlations improved with normalization\n",
    "- Some degraded  \n",
    "- Many remained essentially unchanged\n",
    "\n",
    "## Recommended Implementation:\n",
    "\n",
    "Add this to your notebook after your current analysis:\n",
    "\n",
    " normalization will most significantly impact correlations between variables with very different scales (like comparing a variable ranging 0-100 with one ranging 0.1-0.9), potentially revealing relationships that were masked by scale differences."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
